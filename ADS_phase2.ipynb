{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1X5YMn-TOsXt4qtG7jgJo1M52DN1UztZF","timestamp":1697017882209},{"file_id":"15dmSfPRM-SdRE2ImXyn4Xkj9JevghSyl","timestamp":1697017361917},{"file_id":"1mhmOyNlGUQz5fnlAW_cpPurDjTpBofSh","timestamp":1697015310708}],"authorship_tag":"ABX9TyOByB0wI9KGsW1LZJ1VMdDn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Enhancing IMDb Score Prediction using Advanced Regression Techniques\n","Introduction\n","In this document, we will discuss how to improve the IMDb score prediction for Netflix original shows using advanced regression techniques. The initial code provided employs a RandomForestRegressor, but we will explore two more advanced methods: Gradient Boosting and Neural Networks. These techniques can potentially yield better prediction accuracy compared to a simple RandomForestRegressor.\n","\n","**Problem Statement**\n","The problem at hand is to predict IMDb scores for Netflix original shows based on various features such as the year of release, the country of origin, and more. We want to enhance the prediction accuracy of IMDb scores for these shows.\n","\n","**Proposed Solutions**\n","\n","**Gradient Boosting** is an ensemble learning method that builds a model in a stage-wise fashion. It combines multiple weak learners (typically decision trees) to create a strong predictive model. Here's how you can implement Gradient Boosting:"],"metadata":{"id":"wQS0z1mkE-3B"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","# Build a simple neural network\n","model_nn = keras.Sequential([\n","    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n","    keras.layers.Dense(32, activation='relu'),\n","    keras.layers.Dense(1)  # Output layer\n","])\n","\n","# Compile the model\n","model_nn.compile(optimizer='adam', loss='mean_squared_error')\n","\n","# Train the model\n","model_nn.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n","\n","# Evaluate the model\n","y_pred_nn = model_nn.predict(X_test)\n","mae_nn = mean_absolute_error(y_test, y_pred_nn)\n","mse_nn = mean_squared_error(y_test, y_pred_nn)\n","r2_nn = r2_score(y_test, y_pred_nn)\n","\n","print(\"Neural Network Results:\")\n","print(f\"Mean Absolute Error (MAE): {mae_nn:.2f}\")\n","print(f\"Mean Squared Error (MSE): {mse_nn:.2f}\")\n","print(f\"R-squared (R^2): {r2_nn:.2f}\")\n"],"metadata":{"id":"GmB7EFsVGq7F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.ensemble import GradientBoostingRegressor\n","\n","# Create and train the model\n","model_gb = GradientBoostingRegressor(n_estimators=100, random_state=42)\n","model_gb.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred_gb = model_gb.predict(X_test)\n","\n","# Evaluate the model\n","mae_gb = mean_absolute_error(y_test, y_pred_gb)\n","mse_gb = mean_squared_error(y_test, y_pred_gb)\n","r2_gb = r2_score(y_test, y_pred_gb)\n","\n","print(\"Gradient Boosting Results:\")\n","print(f\"Mean Absolute Error (MAE): {mae_gb:.2f}\")\n","print(f\"Mean Squared Error (MSE): {mse_gb:.2f}\")\n","print(f\"R-squared (R^2): {r2_gb:.2f}\")\n"],"metadata":{"id":"lhhVZNWGGglH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2.\n","***Neural Networks***, particularly deep learning models, have the capacity to capture complex patterns in the data. Here, we'll create a simple feedforward neural network using a library like TensorFlow or Keras:[link text](https://)"],"metadata":{"id":"eSAoojTGGl5m"}},{"cell_type":"markdown","source":[],"metadata":{"id":"OhgwrrJQINSm"}},{"cell_type":"markdown","source":["**Model Evaluation**\n","After implementing Gradient Boosting and Neural Networks, you should evaluate the performance of all three models (RandomForestRegressor, Gradient Boosting, and Neural Network). This will help determine which model provides the best IMDb score predictions.\n","\n","**Conclusion**\n","In this document, we discussed advanced regression techniques for enhancing IMDb score prediction for Netflix original shows. The initial code used RandomForestRegressor, and we explored Gradient Boosting and Neural Networks as potential alternatives. By comparing the results of these models, you can choose the one that provides the best prediction accuracy for your specific dataset.\n","\n","Remember to fine-tune hyperparameters, conduct cross-validation, and explore feature engineering to further improve your IMDb score prediction model."],"metadata":{"id":"3juBjPdlGwbY"}},{"cell_type":"markdown","source":["I have used Graient Boosting technique to boost the performmance.i have provided the code below."],"metadata":{"id":"HDRQLnGUILT_"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import matplotlib.pyplot as plt\n","import xgboost as xgb\n","\n","\n","encodings_to_try = ['utf-8', 'ISO-8859-1', 'latin1', 'cp1252']\n","\n","for encoding in encodings_to_try:\n","    try:\n","        data = pd.read_csv('NetflixOriginals.csv', encoding=encoding)\n","        break\n","    except UnicodeDecodeError:\n","        continue\n","\n","data['IMDB Score'] = data['IMDB Score'].fillna(data['IMDB Score'].mean())\n","\n","genres = data['Genre'].str.get_dummies(',')\n","data = pd.concat([data, genres], axis=1)\n","\n","\n","data['Premiere'] = pd.to_datetime(data['Premiere'])\n","data['PremiereYear'] = data['Premiere'].dt.year\n","data = data.drop(['Title', 'Genre', 'Premiere', 'Runtime', 'Language'], axis=1)\n","\n","X = data.drop(['IMDB Score'], axis=1)\n","y = data['IMDB Score']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","def create_xgb_model(learning_rate=0.1, n_estimators=200, max_depth=3, random_state=42):\n","    return xgb.XGBRegressor(learning_rate=learning_rate, n_estimators=n_estimators, max_depth=max_depth, random_state=random_state)\n","\n","\n","xgb_model = create_xgb_model()\n","\n","\n","param_grid = {\n","    'n_estimators': [100, 200, 300],\n","    'learning_rate': [0.01, 0.1, 0.2],\n","    'max_depth': [3, 4, 5]\n","}\n","\n","\n","grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3)\n","grid_search_result = grid_search.fit(X_train, y_train)\n","\n","\n","best_xgb = grid_search_result.best_estimator_\n","\n","\n","y_pred_xgb = best_xgb.predict(X_test)\n","\n","\n","mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n","mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n","r2_xgb = r2_score(y_test, y_pred_xgb)\n","\n","print(\"Optimal XGBoost Results:\")\n","print(f\"Mean Absolute Error (MAE): {mae_xgb:.2f}\")\n","print(f\"Mean Squared Error (MSE): {mse_xgb:.2f}\")\n","print(f\"R-squared (R^2): {r2_xgb:.2f}\")\n","\n","\n","plt.scatter(y_test, y_pred_xgb)\n","plt.xlabel(\"Actual IMDb Scores\")\n","plt.ylabel(\"Predicted IMDb Scores\")\n","plt.title(\"Actual vs. Predicted IMDb Scores\")\n","plt.show()"],"metadata":{"id":"kcqXXhUdI-at"},"execution_count":null,"outputs":[]}]}